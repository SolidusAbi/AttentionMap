{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "project_dir = os.path.join(os.getcwd(),'..')\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "\n",
    "sparse_dir = os.path.join(project_dir, 'modules/Sparse')\n",
    "if sparse_dir not in sys.path:\n",
    "    sys.path.append(sparse_dir)\n",
    "    \n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "a = torch.rand(1,3,8,12)\n",
    "N,C,H,W = a.size()\n",
    "plt.imshow(a[0,0])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "conv_c = nn.Conv2d(C, 1, 1, bias=False)\n",
    "bn = nn.BatchNorm2d(C)\n",
    "# conv_h = nn.Conv2d(C, C, (1, W), bias=False, groups=C)\n",
    "# conv_w = nn.Conv2d(C, C, (H, 1), bias=True, groups=C)\n",
    "conv_h = nn.Conv2d(C, C, (1, W), bias=False)\n",
    "conv_w = nn.Conv2d(C, C, (H, 1), bias=False)\n",
    "selu = nn.SiLU()\n",
    "sigmoid = nn.Sigmoid()\n",
    "relu6 = nn.ReLU6()\n",
    "# act = nn.Softmax(dim=2)\n",
    "\n",
    "x_h = sigmoid(conv_h(a))\n",
    "x_w = sigmoid(conv_w(a))\n",
    "\n",
    "print(x_h.shape)\n",
    "print(x_w.shape)\n",
    "# print(torch.relu(x_h+x_w).shape)\n",
    "\n",
    "result = relu6(conv_c(x_h*x_w)).detach()\n",
    "# result = torch.softmax(result.view(N,1,-1), dim=2).view(N,1,W,H)\n",
    "plt.imshow(result[0,0])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.mul(result.expand_as(a), a) \n",
    "plt.imshow(f[0,1])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd Option\n",
    "\n",
    "Average as spatial descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "n,c,w,h = a.size()\n",
    "\n",
    "pool_h = nn.AdaptiveAvgPool2d((None, 1))\n",
    "pool_w = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "conv_c = nn.Conv2d(c, 8, 1, bias=True)\n",
    "bn = nn.BatchNorm2d(8)\n",
    "\n",
    "# conv_h = nn.Conv2d(C, 1, (1, W), bias=True)\n",
    "# conv_w = nn.Conv2d(C, 1, (H, 1), bias=True)\n",
    "conv_h = nn.Conv2d(8, c, 1, bias=False)\n",
    "conv_w = nn.Conv2d(8, c, 1, bias=False)\n",
    "\n",
    "# Activation\n",
    "silu = nn.SiLU()\n",
    "act = nn.Softmax(dim=2)\n",
    "sigmoid = nn.Sigmoid()\n",
    "relu = nn.ReLU()\n",
    "\n",
    "x_h_descriptor = pool_h(a)\n",
    "x_w_descriptor = pool_w(a)\n",
    "\n",
    "x_w_descriptor = x_w_descriptor.permute(0, 1, 3, 2)\n",
    "\n",
    "test = torch.cat([x_w_descriptor, x_h_descriptor], dim=2)\n",
    "\n",
    "# test = x_w_descriptor + x_h_descriptor \n",
    "# result = bn(silu(conv_c(test))).detach()\n",
    "result = relu6(bn(conv_c(test))).detach()\n",
    "\n",
    "x_w, x_h = torch.split(result, [h, w], dim=2)\n",
    "x_w = x_w.permute(0, 1, 3, 2)\n",
    "\n",
    "print(a.shape)\n",
    "print(x_h.shape)\n",
    "print(x_w.shape)\n",
    "\n",
    "x_h = sigmoid(conv_h(x_h))\n",
    "x_w = sigmoid(conv_w(x_w))\n",
    "# x_h = (conv_h(x_h))\n",
    "# x_w = (conv_w(x_w))\n",
    "\n",
    "result = (a*x_w * x_h).detach()\n",
    "\n",
    "# result = act(conv_c((x_h+x_w))).detach()\n",
    "# # result = torch.softmax(result.view(N,1,-1), dim=2).view(N,1,W,H)\n",
    "plt.figure(figsize=(16,4))\n",
    "for i in range(c):\n",
    "    plt.subplot(1,c,i+1)\n",
    "    plt.imshow(result[0,i])\n",
    "    plt.colorbar()\n",
    "\n",
    "    \n",
    "# plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[result<.1].shape)\n",
    "print(result[result>=.1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AttentionMap.LocalAttention import CoordinateAttentionBlock, CoordAttentionConv\n",
    "\n",
    "test = CoordinateAttentionBlock(3, 16, 4)\n",
    "test(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test(a).detach()\n",
    "\n",
    "print(result[result<.25].shape)\n",
    "print(result[result>=.25].shape)\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "for i in range(result.size(1)):\n",
    "    plt.subplot(2,result.size(1)//2,i+1)\n",
    "    plt.imshow(result[0,i])\n",
    "    plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CoordAttentionConv(3, 8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(a).detach()\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "for i in range(result.size(1)):\n",
    "    plt.subplot(2,result.size(1)//2,i+1)\n",
    "    plt.imshow(result[0,i])\n",
    "    plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "class CoordAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_channels: int, img_size: tuple) -> None:\n",
    "        super(CoordAttentionBlock, self).__init__()\n",
    "        C = n_channels\n",
    "        H, W = img_size\n",
    "        reduction = 8\n",
    "\n",
    "        out_channels = C // reduction\n",
    "        \n",
    "        self.conv_c = nn.Conv2d(out_channels, 1, 1, bias=False)\n",
    "        # self.conv_h = nn.Conv2d(C, 1, (1, W), bias=False)\n",
    "        # self.conv_w = nn.Conv2d(C, 1, (H, 1), bias=False)\n",
    "        # self.conv_h = nn.Conv2d(C, C, (1, W), bias=False, groups=C)\n",
    "        # self.conv_w = nn.Conv2d(C, C, (H, 1), bias=False, groups=C)\n",
    "        self.conv_h = nn.Sequential(\n",
    "            nn.Conv2d(C, out_channels, (1, W), bias=False, groups=out_channels),\n",
    "            # nn.BatchNorm2d(C),\n",
    "            # nn.SiLU()\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv_w = nn.Sequential(\n",
    "            nn.Conv2d(C, out_channels, (H, 1), bias=False, groups=out_channels),\n",
    "            # nn.BatchNorm2d(C),\n",
    "            # nn.SiLU()\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # self.act = nn.Softmax(1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        N,C,H,W = x.size()\n",
    "        x_h = self.conv_h(x)\n",
    "        x_w = self.conv_w(x)\n",
    "\n",
    "        c = self.conv_c(x_h+x_w)\n",
    "        # c = self.conv_c(x_h+torch.sigmoid(x_w))\n",
    "        # c = torch.relu(x_h+x_w)\n",
    "        \n",
    "        a = torch.softmax(c.view(N,1,-1), dim=2).view(N,1,W,H)\n",
    "        f = torch.mul(a.expand_as(x), x)\n",
    "        output = f.view(N,C,-1).sum(dim=2)\n",
    "        \n",
    "        return c, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvTestBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, img_size: tuple) -> None:\n",
    "        super(ConvTestBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(*[\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        ])\n",
    "\n",
    "        self.coordAttBlock = CoordAttentionBlock(out_channels, img_size)\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        attention = self.coordAttBlock(x)\n",
    "        \n",
    "        return torch.mul(attention.expand_as(x), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, ) -> None:\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = nn.Sequential(*[\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        ])\n",
    "        self.l2 = nn.Sequential(*[\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        ])\n",
    "        self.l3 = nn.Sequential(*[\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        ])\n",
    "\n",
    "        self.classifier = nn.Sequential(*[\n",
    "            nn.Linear(64+128+256,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,10)\n",
    "        ])\n",
    "\n",
    "        img_size = 32\n",
    "        self.coordAttBlock_l1 = CoordAttentionBlock(64, (int(img_size/2**1), int(img_size/2**1)) )\n",
    "        self.coordAttBlock_l2 = CoordAttentionBlock(128, (int(img_size/2**2), int(img_size/2**2)) )\n",
    "        self.coordAttBlock_l3 = CoordAttentionBlock(256, (int(img_size/2**3), int(img_size/2**3)) )\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        x_l1 = self.l1(x)\n",
    "        x_l2 = self.l2(x_l1)\n",
    "        x_l3 = self.l3(x_l2)\n",
    "\n",
    "        c1, g1 = self.coordAttBlock_l1(x_l1)\n",
    "        c2, g2 = self.coordAttBlock_l2(x_l2)\n",
    "        c3, g3 = self.coordAttBlock_l3(x_l3)\n",
    "\n",
    "        g = torch.cat((g1,g2,g3), dim=1)\n",
    "        y_hat = self.classifier(g)\n",
    "\n",
    "        return (y_hat, c1, c2, c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AttentionMap.LocalAttention import CoordinateAttentionBlock\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, ) -> None:\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = nn.Sequential(*[\n",
    "            nn.Conv2d(3, 12, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        ])\n",
    "        self.l2 = nn.Sequential(*[\n",
    "            nn.Conv2d(12, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        ])\n",
    "        self.l3 = nn.Sequential(*[\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        ])\n",
    "\n",
    "        self.classifier = nn.Sequential(*[\n",
    "            nn.Linear(12+64+128,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,10)\n",
    "        ])\n",
    "\n",
    "        img_size = 32\n",
    "        self.coordAttBlock_l1 = CoordAttentionBlock(12, (int(img_size/2**1), int(img_size/2**1)) )\n",
    "        self.coordAttBlock_l2 = CoordAttentionBlock(64, (int(img_size/2**2), int(img_size/2**2)) )\n",
    "        self.coordAttBlock_l3 = CoordAttentionBlock(128, (int(img_size/2**3), int(img_size/2**3)) )\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        x_l1 = self.l1(x)\n",
    "        x_l2 = self.l2(x_l1)\n",
    "        x_l3 = self.l3(x_l2)\n",
    "\n",
    "        c1, g1 = self.coordAttBlock_l1(x_l1)\n",
    "        c2, g2 = self.coordAttBlock_l2(x_l2)\n",
    "        c3, g3 = self.coordAttBlock_l3(x_l3)\n",
    "\n",
    "        g = torch.cat((g1,g2,g3), dim=1)\n",
    "        y_hat = self.classifier(g)\n",
    "\n",
    "        return (y_hat, c1, c2, c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "\n",
    "transform = Compose([Resize(32), ToTensor()])\n",
    "# train_dataset = MNIST('dataset/', train=True, transform=transform, download=True)\n",
    "train_dataset = CIFAR10('dataset/', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# test_dataset = MNIST('dataset/', train=False, transform=transform, download=True)\n",
    "test_dataset = CIFAR10('dataset/', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "img_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.utils as utils\n",
    "from AttentionMap.utils import visualize_attention\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = Model()\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "tb_writer = SummaryWriter('log/CoordAttention/CIFAR')\n",
    "running_avg_accuracy = 0\n",
    "step = 0\n",
    "\n",
    "# TMP\n",
    "log_images = True\n",
    "n_epoch = 50\n",
    "\n",
    "epoch_iterator = tqdm(\n",
    "        range(n_epoch),\n",
    "        leave=True,\n",
    "        unit=\"epoch\",\n",
    "        postfix={\"tls\": \"%.4f\" % 1},\n",
    "    )\n",
    "\n",
    "for epoch in epoch_iterator:\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        # log scalars\n",
    "        images_disp = []\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "                images_test, labels_test = data\n",
    "                images_test, labels_test = images_test.to(device), labels_test.to(device)\n",
    "                pred_test, c1, c2, c3 = model(images_test)\n",
    "                predict = torch.argmax(pred_test, 1)\n",
    "                total += labels_test.size(0)\n",
    "                correct += torch.eq(predict, labels_test).sum().double().item()\n",
    "                \n",
    "        tb_writer.add_scalar('test/accuracy', correct/total, epoch)\n",
    "\n",
    "        n_rows=4\n",
    "        n_images = 24\n",
    "        activation = 'softmax'\n",
    "        # C1\n",
    "        scale_factor = 2**1\n",
    "        vis = visualize_attention(n_rows, images_test[:n_images], c1[:n_images], scale_factor, activation=activation)\n",
    "        tb_writer.add_image('Attention/C1', vis, epoch)\n",
    "\n",
    "        # C2\n",
    "        scale_factor = 2**2\n",
    "        vis = visualize_attention(n_rows, images_test[:n_images], c2[:n_images], scale_factor, activation=activation)\n",
    "        tb_writer.add_image('Attention/C2', vis, epoch)\n",
    "\n",
    "        # C3\n",
    "        scale_factor = 2**3\n",
    "        vis = visualize_attention(n_rows, images_test[:n_images], c3[:n_images], scale_factor, activation=activation)\n",
    "        tb_writer.add_image('Attention/C3', vis, epoch)\n",
    "\n",
    "    model.train()\n",
    "    for idx, (inputs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        pred, __, __, __ = model(inputs)\n",
    "\n",
    "        loss = criterion(pred, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 250 == 0:\n",
    "            model.eval()\n",
    "            pred, __, __, __ = model(inputs)\n",
    "            predict = torch.argmax(pred, 1)\n",
    "            total = targets.size(0)\n",
    "            correct = torch.eq(predict, targets).sum().double().item()\n",
    "            accuracy = correct / total\n",
    "            running_avg_accuracy = 0.6*running_avg_accuracy + 0.4*accuracy\n",
    "            tb_writer.add_scalar('train/loss', loss.item(), step)\n",
    "            tb_writer.add_scalar('train/accuracy', accuracy, step)\n",
    "            tb_writer.add_scalar('train/running_avg_accuracy', running_avg_accuracy, step)\n",
    "            step += 1\n",
    "\n",
    "            epoch_iterator.set_postfix(tls=\"%.4f\" % loss.item())            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(c1[0,0].detach().cpu())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(c2[0,0].detach().cpu())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(c3[0,0].detach().cpu())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(input[0,0].detach().cpu())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Attention Block\n",
    "\n",
    "Se utiliza kernels de distintos tamaños para obtener descriptores horizontales y verticales, el propósito es obtener descriptores que definan la información espacial de forma complementaria para luego unificar dicha información mediante la adición (o concatenación) de ambos descriptores.\n",
    "\n",
    "De todas formas, no se puede ignorar la información de los canales de cada input. Estos podrían afectar drásticamente a la los descriptores espaciales. Por este motivo, se han aplicado DepthWise Separable convolutions para que cada descriptor no se base toda su información teniendo en cuenta todos los canales, sino un subconjunto de ellos. (Es necesario desarrollar esto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalAttentionBlock(nn.Module):\n",
    "    def __init__(self, img_size: tuple, in_channels: int, reduction_rate: int, groups=False, bias=True) -> None:\n",
    "        super(CoordinateAttentionBlock, self).__init__()\n",
    "        out_channels = max(8, in_channels // reduction_rate)\n",
    "        H, W = img_size\n",
    "\n",
    "        self.conv_h = nn.Sequential(\n",
    "            nn.Conv2d(C, out_channels, (1, W), bias=bias, groups=out_channels if groups else 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        self.conv_w = nn.Sequential(\n",
    "            nn.Conv2d(C, out_channels, (H, 1), bias=bias, groups=out_channels if groups else 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        self.att = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, in_channels, 1, bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_h = self.conv_h(x) # Height descriptor\n",
    "        x_w = self.conv_w(x) # Weight descriptor\n",
    "\n",
    "        # Coordinate attention\n",
    "        coordAtt = self.att(x_h+x_w)\n",
    "        # TODO: Concatenate x_h and x_w\n",
    "        \n",
    "        return coordAtt  \n",
    "\n",
    "class CoordAttConv(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int, padding: int, groups: int, bias: bool) -> None:\n",
    "        super(CoordAttConv, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        \n",
    "        # self.att_block = CoordinateAttentionBlock(out_channels, out_channels, att_reduction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "resnet18 = models.resnet18()\n",
    "resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27375/2484855771.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv3x3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBasicBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mexpansion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "from torchvision.models.resnet import conv3x3\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.ResNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 16\n",
    "reduction_rate = 4\n",
    "out_channels = in_channels // reduction_rate\n",
    "a = torch.rand((1,in_channels,4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_h = nn.Conv2d(in_channels, out_channels, kernel_size=(1,4), groups=out_channels, bias=False)\n",
    "conv_w = nn.Conv2d(in_channels, out_channels, kernel_size=(4,1), groups=out_channels, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silu = nn.SiLU()\n",
    "desc_h = silu(conv_h(a))\n",
    "desc_w = silu(conv_w(a))\n",
    "\n",
    "desc = (desc_h + desc_w).detach()\n",
    "\n",
    "conv_c = nn.Conv2d(out_channels, in_channels, kernel_size=(1,1), bias=False)\n",
    "result = conv_c(desc).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find minimum of minima & maximum of maxima\n",
    "minmin = torch.min(result)\n",
    "\n",
    "for i in range(out_channels):\n",
    "    plt.subplot(out_channels, 1, i+1)\n",
    "    plt.imshow(desc[0,i], vmin=torch.min(desc), vmax=torch.max(desc))\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,10))\n",
    "# test = torch.softmax(result.flatten(2), dim=2)\n",
    "# test = test.reshape(result.shape)\n",
    "for i in range(in_channels):\n",
    "    plt.subplot(1, in_channels, i+1)\n",
    "    plt.imshow(torch.sigmoid(result[0,i]))\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.softmax(result.flatten(2), dim=2)\n",
    "test = test.reshape(1,16,4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('HySpecLab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc81a3ec444beb1d5a523daf231afa571e79be8a57abb6fe0028623a3d4d7136"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
