{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Attention Block\n",
    "\n",
    "Propuesta alternativa al CoordinateAttention.\n",
    "\n",
    "Se utiliza kernels de distintos tamaños para obtener descriptores horizontales y verticales, el propósito es obtener descriptores que definan la información espacial de forma complementaria para luego unificar dicha información mediante la adición (o concatenación) de ambos descriptores.\n",
    "\n",
    "De todas formas, no se puede ignorar la información de los canales de cada input. Estos podrían afectar drásticamente a la los descriptores espaciales. Por este motivo, se han aplicado DepthWise Separable convolutions para que cada descriptor no se base toda su información teniendo en cuenta todos los canales, sino un subconjunto de ellos. (Es necesario desarrollar esto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Block V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalAttentionBlock(nn.Module):\n",
    "    def __init__(self, img_size: tuple, in_channels: int, reduction_rate: int, groups=False, bias=True) -> None:\n",
    "        super(ConvolutionalAttentionBlock, self).__init__()\n",
    "        mip = max(8, in_channels // reduction_rate)\n",
    "        \n",
    "        self.squeeze_h = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((None, 1)),\n",
    "            nn.Conv2d(in_channels, mip, 1, bias=False),\n",
    "            nn.BatchNorm2d(mip),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        self.squeeze_w = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, None)),\n",
    "            nn.Conv2d(in_channels, mip, 1, bias=False),\n",
    "            nn.BatchNorm2d(mip),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Conv2d(mip, in_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_h = self.squeeze_h(x) # Height descriptor shape: (C x W x 1)\n",
    "        x_w = self.squeeze_w(x) # Width descriptor shape: (C x 1 x H)\n",
    "\n",
    "        # Coordinate attention\n",
    "        coordAtt = self.excitation(x_h+x_w)\n",
    "        # TODO: Concatenate x_h and x_w\n",
    "        \n",
    "        return coordAtt  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Block V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalAttentionBlock(nn.Module):\n",
    "    def __init__(self, img_size: tuple, in_channels: int, reduction_rate: int, groups=True, bias=False) -> None:\n",
    "        super(ConvolutionalAttentionBlock, self).__init__()\n",
    "        mip = max(8, in_channels // reduction_rate)\n",
    "        H, W = img_size\n",
    "\n",
    "        self.conv_h = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mip, (1, W), bias=bias, groups=mip if groups else 1),\n",
    "            nn.BatchNorm2d(mip),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        self.conv_w = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mip, (H, 1), bias=bias, groups=mip if groups else 1),\n",
    "            nn.BatchNorm2d(mip),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        self.att = nn.Sequential(\n",
    "            nn.Conv2d(mip, in_channels, 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_h = self.conv_h(x) # Height descriptor\n",
    "        x_w = self.conv_w(x) # Width descriptor\n",
    "\n",
    "        # Coordinate attention\n",
    "        coordAtt = self.att(x_h+x_w)\n",
    "        # TODO: Concatenate x_h and x_w\n",
    "        \n",
    "        return coordAtt  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESNet-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Block with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import BasicBlock\n",
    "\n",
    "class ResAttentionBlock(BasicBlock):\n",
    "    def __init__(self, img_size: tuple, inplanes:int, planes:int, stride=1, downsample=None, \n",
    "                att_reduction=8, att_groups=True, att_bias=True, **kargs):\n",
    "                 \n",
    "        super(ResAttentionBlock, self).__init__(inplanes, planes, stride, downsample)\n",
    "\n",
    "        self.attention = ConvolutionalAttentionBlock(img_size, planes, att_reduction, \n",
    "                    groups=att_groups, bias=att_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        att = self.attention(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        res = self.relu((att*out) + identity)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import conv1x1\n",
    "from torchvision.models import ResNet\n",
    "import numpy as np\n",
    "\n",
    "class ResNet_Attention(ResNet):\n",
    "    def __init__(self, img_size:tuple, block:nn.Module, layers:list, num_classes=1000, **kargs):\n",
    "        super(ResNet_Attention, self).__init__(BasicBlock, layers, num_classes, kargs)\n",
    "\n",
    "        if not isinstance(img_size, np.ndarray):\n",
    "            img_size = np.array(img_size)\n",
    "\n",
    "        self.inplanes = 64 # Because in super init it has been set to 512\n",
    "        self.layer1 = self._make_attention_layer(tuple(img_size // (2**2)), block, 64, layers[0], **kargs)\n",
    "        self.layer2 = self._make_attention_layer(tuple(img_size // (2**3)), block, 128, layers[1], stride=2,\n",
    "                                       dilate=False, **kargs)\n",
    "        self.layer3 = self._make_attention_layer(tuple(img_size // (2**4)), block, 256, layers[2], stride=2,\n",
    "                                       dilate=False, **kargs)\n",
    "        self.layer4 = self._make_attention_layer(tuple(img_size // (2**5)), block, 512, layers[3], stride=2,\n",
    "                                       dilate=False, **kargs)\n",
    "        \n",
    "    def _make_attention_layer(self, input_size, block, planes, blocks, stride=1, dilate=False, **kargs):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(input_size, self.inplanes, planes, stride, downsample, \n",
    "                    groups=self.groups, base_width=self.base_width, dilation=previous_dilation,\n",
    "                    norm_layer=norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(input_size, self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import BasicBlock\n",
    "\n",
    "# ResNet-18 config\n",
    "# https://github.com/pytorch/vision/blob/28557e0cfe9113a5285330542264f03e4ba74535/torchvision/models/resnet.py#L649-L670\n",
    "resnet_18 = ResNet_Attention((128,128), ResAttentionBlock, [2,2,2,2], num_classes=1000)\n",
    "result = resnet_18(torch.rand(1,3,128,128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "\n",
    "transform = Compose([Resize(128), ToTensor()])\n",
    "train_dataset = CIFAR10('dataset/', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "test_dataset = CIFAR10('dataset/', train=False, transform=transform, download=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "img_size = (128,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, n_epoch, train_loader, test_loader, exp_name):\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "\n",
    "    tb_writer = SummaryWriter('log/{}'.format(exp_name))\n",
    "    running_avg_accuracy = 0\n",
    "    step = 0\n",
    "\n",
    "    epoch_iterator = tqdm(\n",
    "            range(n_epoch),\n",
    "            leave=True,\n",
    "            unit=\"epoch\",\n",
    "            postfix={\"tls\": \"%.4f\" % 1},\n",
    "        )\n",
    "\n",
    "    for epoch in epoch_iterator:\n",
    "        model.eval()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        model.train()\n",
    "        for idx, (inputs, targets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            pred = model(inputs)\n",
    "\n",
    "            loss = criterion(pred, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if idx % 250 == 0:\n",
    "                model.eval()\n",
    "                pred = model(inputs)\n",
    "                predict = torch.argmax(pred, 1)\n",
    "                total = targets.size(0)\n",
    "                correct = torch.eq(predict, targets).sum().double().item()\n",
    "                accuracy = correct / total\n",
    "                running_avg_accuracy = 0.6*running_avg_accuracy + 0.4*accuracy\n",
    "                tb_writer.add_scalar('train/loss', loss.item(), step)\n",
    "                tb_writer.add_scalar('train/accuracy', accuracy, step)\n",
    "                tb_writer.add_scalar('train/running_avg_accuracy', running_avg_accuracy, step)\n",
    "                step += 1\n",
    "\n",
    "                epoch_iterator.set_postfix(tls=\"%.4f\" % loss.item())      \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_loader, 0):\n",
    "                    images_test, labels_test = data\n",
    "                    images_test, labels_test = images_test.to(device), labels_test.to(device)\n",
    "                    pred_test = model(images_test)\n",
    "                    predict = torch.argmax(pred_test, 1)\n",
    "                    total += labels_test.size(0)\n",
    "                    correct += torch.eq(predict, labels_test).sum().double().item()\n",
    "                    \n",
    "            tb_writer.add_scalar('test/accuracy', correct/total, epoch)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "resnet_18 = resnet18(num_classes=10)\n",
    "train(resnet_18, 50, train_loader, test_loader, 'ConvAttention/CIFAR/original_resnet18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_18 = ResNet_Attention((128,128), ResAttentionBlock, [2, 2, 2, 2], num_classes=10)\n",
    "train(resnet_18, 50, train_loader, test_loader, 'ConvAttention/CIFAR/attention_v2_resnet18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('DeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3359c77a731973c13502bd1902dfd07916e9bbb09ad2d14e58e7b1c01c627b1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
