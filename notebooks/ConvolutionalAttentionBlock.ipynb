{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Attention Block\n",
    "\n",
    "Propuesta alternativa al CoordinateAttention.\n",
    "\n",
    "Se utiliza kernels de distintos tamaños para obtener descriptores horizontales y verticales, el propósito es obtener descriptores que definan la información espacial de forma complementaria para luego unificar dicha información mediante la adición (o concatenación) de ambos descriptores.\n",
    "\n",
    "De todas formas, no se puede ignorar la información de los canales de cada input. Estos podrían afectar drásticamente a la los descriptores espaciales. Por este motivo, se han aplicado DepthWise Separable convolutions para que cada descriptor no se base toda su información teniendo en cuenta todos los canales, sino un subconjunto de ellos. (Es necesario desarrollar esto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "project_dir = os.path.join(os.getcwd(),'..')\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "\n",
    "sparse_dir = os.path.join(project_dir, 'modules/Sparse')\n",
    "if sparse_dir not in sys.path:\n",
    "    sys.path.append(sparse_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Block V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalAttentionBlock(nn.Module):\n",
    "    def __init__(self, img_size: tuple, in_channels: int, reduction_rate: int, groups=False, bias=True) -> None:\n",
    "        super(ConvolutionalAttentionBlock, self).__init__()\n",
    "        mip = max(8, in_channels // reduction_rate)\n",
    "        \n",
    "        self.squeeze_h = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((None, 1)),\n",
    "            nn.Conv2d(in_channels, mip, 1, bias=False),\n",
    "            nn.BatchNorm2d(mip),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        self.squeeze_w = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, None)),\n",
    "            nn.Conv2d(in_channels, mip, 1, bias=False),\n",
    "            nn.BatchNorm2d(mip),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Conv2d(mip, in_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_h = self.squeeze_h(x) # Height descriptor shape: (C x W x 1)\n",
    "        x_w = self.squeeze_w(x) # Width descriptor shape: (C x 1 x H)\n",
    "\n",
    "        # Coordinate attention\n",
    "        coordAtt = self.excitation(x_h+x_w)\n",
    "        # TODO: Concatenate x_h and x_w\n",
    "        \n",
    "        return coordAtt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Block V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalAttentionBlock(nn.Module):\n",
    "    def __init__(self, img_size: tuple, in_channels: int, reduction_rate: int, groups=True, bias=False) -> None:\n",
    "        super(ConvolutionalAttentionBlock, self).__init__()\n",
    "        mip = max(4, in_channels // reduction_rate)\n",
    "        H, W = img_size\n",
    "\n",
    "        self.conv_h = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mip, (1, W), bias=bias, groups=mip if groups else 1),\n",
    "            nn.BatchNorm2d(mip),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        self.conv_w = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mip, (H, 1), bias=bias, groups=mip if groups else 1),\n",
    "            nn.BatchNorm2d(mip),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        self.att = nn.Sequential(\n",
    "            nn.Conv2d(mip, in_channels, 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_h = self.conv_h(x) # Height descriptor\n",
    "        x_w = self.conv_w(x) # Width descriptor\n",
    "\n",
    "        # Coordinate attention\n",
    "        coordAtt = self.att(x_h+x_w)\n",
    "        # TODO: Concatenate x_h and x_w\n",
    "        \n",
    "        return coordAtt  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Block V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalAttentionBlock(nn.Module):\n",
    "    def __init__(self, img_size: tuple, in_channels: int, reduction_rate: int, groups=True, bias=True) -> None:\n",
    "        super(ConvolutionalAttentionBlock, self).__init__()\n",
    "        mip = max(8, in_channels // reduction_rate)\n",
    "        \n",
    "        self.squeeze_h = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((None, 1)),\n",
    "            nn.Conv2d(in_channels, mip, 1, bias=False, groups=mip if groups else 1),\n",
    "            nn.BatchNorm2d(mip),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        self.squeeze_w = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, None)),\n",
    "            nn.Conv2d(in_channels, mip, 1, bias=False, groups=mip if groups else 1),\n",
    "            nn.BatchNorm2d(mip),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Conv2d(mip, in_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_h = self.squeeze_h(x) # Height descriptor shape: (C x W x 1)\n",
    "        x_w = self.squeeze_w(x) # Width descriptor shape: (C x 1 x H)\n",
    "\n",
    "        # Coordinate attention\n",
    "        coordAtt = self.excitation(x_h+x_w)\n",
    "        # TODO: Concatenate x_h and x_w\n",
    "        \n",
    "        return coordAtt   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Block V4\n",
    "\n",
    "Sparse spatial solution, no parece ir bien...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalAttentionBlock(nn.Module):\n",
    "    def __init__(self, img_size: tuple, in_channels: int, reduction_rate: int, groups=False, bias=True) -> None:\n",
    "        super(ConvolutionalAttentionBlock, self).__init__()\n",
    "        mip = max(8, in_channels // reduction_rate)\n",
    "        \n",
    "        self.squeeze_h = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((None, 1)),\n",
    "            nn.Conv2d(in_channels, mip, 1, bias=False),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.squeeze_w = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, None)),\n",
    "            nn.Conv2d(in_channels, mip, 1, bias=False),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Conv2d(mip, in_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_h = self.squeeze_h(x) # Height descriptor shape: (C x W x 1)\n",
    "        x_w = self.squeeze_w(x) # Width descriptor shape: (C x 1 x H)\n",
    "\n",
    "        # Coordinate attention\n",
    "        coordAtt = self.excitation(x_h+x_w)\n",
    "        # TODO: Concatenate x_h and x_w\n",
    "        \n",
    "        return coordAtt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AttentionMap.LocalAttention import CoordinateAttentionBlock\n",
    "class ConvolutionalAttentionBlock(nn.Module):\n",
    "    def __init__(self, img_size: tuple, in_channels: int, reduction_rate: int, groups=True, bias=False) -> None:\n",
    "        super(ConvolutionalAttentionBlock, self).__init__()\n",
    "        self.coordAtt = CoordinateAttentionBlock(in_channels, in_channels, reduction_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.coordAtt(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squeeze-And-Excitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AttentionMap.LocalAttention import SqueezeAndExcitationBlock\n",
    "class ConvolutionalAttentionBlock(nn.Module):\n",
    "    def __init__(self, img_size: tuple, in_channels: int, reduction_rate: int, groups=False, bias=False) -> None:\n",
    "        super(ConvolutionalAttentionBlock, self).__init__()\n",
    "        self.sae = SqueezeAndExcitationBlock(in_channels, reduction_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.sae(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBAM: Convolutional Block Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AttentionMap.LocalAttention import CBAM\n",
    "class ConvolutionalAttentionBlock(nn.Module):\n",
    "    def __init__(self, img_size: tuple, in_channels: int, reduction_rate: int, groups=False, bias=False) -> None:\n",
    "        super(ConvolutionalAttentionBlock, self).__init__()\n",
    "        self.cbam = CBAM(in_channels, reduction_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.cbam(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial pyramid pooling (SPP) attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AttentionMap.LocalAttention import SPPAttentionBlock\n",
    "\n",
    "class ConvolutionalAttentionBlock(nn.Module):\n",
    "    def __init__(self, img_size: tuple, in_channels: int, reduction_rate: int, groups=False, bias=False) -> None:\n",
    "        super(ConvolutionalAttentionBlock, self).__init__()\n",
    "        self.spp = SPPAttentionBlock(in_channels, reduction_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.spp(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolution Guided Pooling (RGP) approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AttentionMap.LocalAttention import RGPAttentionBlock\n",
    "\n",
    "class ConvolutionalAttentionBlock(nn.Module):\n",
    "    def __init__(self, img_size: tuple, in_channels: int, reduction_rate: int, groups=False, bias=False) -> None:\n",
    "        super(ConvolutionalAttentionBlock, self).__init__()\n",
    "        self.spp = RGPAttentionBlock(img_size, in_channels, reduction_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.spp(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESNet-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Block with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import BasicBlock\n",
    "\n",
    "class ResAttentionBlock(BasicBlock):\n",
    "    def __init__(self, img_size: tuple, inplanes:int, planes:int, stride=1, downsample=None, \n",
    "                att_reduction=8, att_groups=True, att_bias=True, **kargs):\n",
    "                 \n",
    "        super(ResAttentionBlock, self).__init__(inplanes, planes, stride, downsample)\n",
    "\n",
    "        self.attention = ConvolutionalAttentionBlock(img_size, planes, att_reduction, \n",
    "                    groups=att_groups, bias=att_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        att = self.attention(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        res = self.relu((att*out) + identity)\n",
    "        # res = self.relu(att + identity) # Para SAE!\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import conv1x1\n",
    "from torchvision.models import ResNet\n",
    "import numpy as np\n",
    "\n",
    "class ResNet_Attention(ResNet):\n",
    "    def __init__(self, img_size:tuple, block:nn.Module, layers:list, num_classes=1000, **kargs):\n",
    "        super(ResNet_Attention, self).__init__(BasicBlock, layers, num_classes, kargs)\n",
    "\n",
    "        if not isinstance(img_size, np.ndarray):\n",
    "            img_size = np.array(img_size)\n",
    "\n",
    "        self.inplanes = 64 # Because in super init it has been set to 512\n",
    "        self.layer1 = self._make_attention_layer(tuple(img_size // (2**2)), block, 64, layers[0], **kargs)\n",
    "        self.layer2 = self._make_attention_layer(tuple(img_size // (2**3)), block, 128, layers[1], stride=2,\n",
    "                                       dilate=False, **kargs)\n",
    "        self.layer3 = self._make_attention_layer(tuple(img_size // (2**4)), block, 256, layers[2], stride=2,\n",
    "                                       dilate=False, **kargs)\n",
    "        self.layer4 = self._make_attention_layer(tuple(img_size // (2**5)), block, 512, layers[3], stride=2,\n",
    "                                       dilate=False, **kargs)\n",
    "        \n",
    "    def _make_attention_layer(self, input_size, block, planes, blocks, stride=1, dilate=False, **kargs):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(input_size, self.inplanes, planes, stride, downsample, \n",
    "                    groups=self.groups, base_width=self.base_width, dilation=previous_dilation,\n",
    "                    norm_layer=norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(input_size, self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import BasicBlock\n",
    "\n",
    "# ResNet-18 config\n",
    "# https://github.com/pytorch/vision/blob/28557e0cfe9113a5285330542264f03e4ba74535/torchvision/models/resnet.py#L649-L670\n",
    "resnet_18 = ResNet_Attention((128,128), ResAttentionBlock, [2,2,2,2], num_classes=1000)\n",
    "result = resnet_18(torch.rand(3,3,128,128))\n",
    "resnet_18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "\n",
    "transform = Compose([Resize(128), ToTensor()])\n",
    "train_dataset = CIFAR10('dataset/', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "test_dataset = CIFAR10('dataset/', train=False, transform=transform, download=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "img_size = (128,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, n_epoch, train_loader, test_loader, exp_name):\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "\n",
    "    tb_writer = SummaryWriter('log/{}'.format(exp_name))\n",
    "    running_avg_accuracy = 0\n",
    "    step = 0\n",
    "\n",
    "    epoch_iterator = tqdm(\n",
    "            range(n_epoch),\n",
    "            leave=True,\n",
    "            unit=\"epoch\",\n",
    "            postfix={\"tls\": \"%.4f\" % 1},\n",
    "        )\n",
    "\n",
    "    for epoch in epoch_iterator:\n",
    "        model.eval()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        model.train()\n",
    "        for idx, (inputs, targets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            pred = model(inputs)\n",
    "\n",
    "            loss = criterion(pred, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if idx % 250 == 0:\n",
    "                model.eval()\n",
    "                pred = model(inputs)\n",
    "                predict = torch.argmax(pred, 1)\n",
    "                total = targets.size(0)\n",
    "                correct = torch.eq(predict, targets).sum().double().item()\n",
    "                accuracy = correct / total\n",
    "                running_avg_accuracy = 0.6*running_avg_accuracy + 0.4*accuracy\n",
    "                tb_writer.add_scalar('train/loss', loss.item(), step)\n",
    "                tb_writer.add_scalar('train/accuracy', accuracy, step)\n",
    "                tb_writer.add_scalar('train/running_avg_accuracy', running_avg_accuracy, step)\n",
    "                step += 1\n",
    "\n",
    "                epoch_iterator.set_postfix(tls=\"%.4f\" % loss.item())      \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_loader, 0):\n",
    "                    images_test, labels_test = data\n",
    "                    images_test, labels_test = images_test.to(device), labels_test.to(device)\n",
    "                    pred_test = model(images_test)\n",
    "                    predict = torch.argmax(pred_test, 1)\n",
    "                    total += labels_test.size(0)\n",
    "                    correct += torch.eq(predict, labels_test).sum().double().item()\n",
    "                    \n",
    "            tb_writer.add_scalar('test/accuracy', correct/total, epoch)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.models import resnet18\n",
    "# resnet_18 = resnet18(num_classes=10)\n",
    "# train(resnet_18, 50, train_loader, test_loader, 'ConvAttention/CIFAR/original_resnet18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_18 = ResNet_Attention((128,128), ResAttentionBlock, [2, 2, 2, 2], num_classes=10)\n",
    "train(resnet_18, 50, train_loader, test_loader, 'ConvAttention/CIFAR/spp_resnet18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.set_grad_enabled(False):\n",
    "    x, y = next(iter(test_loader))\n",
    "    x = x.cuda()\n",
    "    x = resnet_18.conv1(x)\n",
    "    x = resnet_18.bn1(x)\n",
    "    x = resnet_18.relu(x)\n",
    "    x = resnet_18.maxpool(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test = resnet_18.layer1[0].conv1(x)\n",
    "    test = resnet_18.layer1[0].bn1(test)\n",
    "    test = resnet_18.layer1[0].relu(test)\n",
    "    test = resnet_18.layer1[0].conv2(test)\n",
    "    test = resnet_18.layer1[0].bn2(test)\n",
    "\n",
    "    attention = resnet_18.layer1[0].attention(test)\n",
    "# test = resnet_18.layer1[0].conv2(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(12,12))\n",
    "for idx in range(64):\n",
    "    plt.subplot(8, 8, idx+1)\n",
    "    plt.imshow(attention[1,idx].cpu(), vmin=0, vmax=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "for idx in range(64):\n",
    "    print(entropy(attention[0,idx].flatten().cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_values = []\n",
    "for i in range(64):\n",
    "    entropy_values.append(entropy(attention[:, i].flatten(1).mean(axis=1).cpu()))\n",
    "\n",
    "plt.plot(entropy_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = RGPAttentionBlock((64,128), 128)\n",
    "\n",
    "a = torch.rand(3, 128, 64, 128)\n",
    "test(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageNet\n",
    "\n",
    "imagenet_dir = '/home/ahguedes/Workspace/External/dataset/ImageNet/'\n",
    "ImageNet(imagenet_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "from torchvision.models.vgg import VGG\n",
    "from AttentionMap.LocalAttention import SqueezeAndExcitationBlock, CBAM\n",
    "\n",
    "class ConvolutionalAttentionBlock(nn.Module):\n",
    "    def __init__(self, img_size: tuple, in_channels: int, reduction_rate: int, groups=False, bias=False) -> None:\n",
    "        super(ConvolutionalAttentionBlock, self).__init__()\n",
    "        # self.sae = SqueezeAndExcitationBlock(in_channels, reduction_rate)\n",
    "        self.att = CBAM(in_channels, reduction_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # return self.sae(x) \n",
    "        return x * self.att(x)\n",
    "\n",
    "class VGG16AttentionFeatures(nn.Module):\n",
    "    def __init__(self, img_size = (128,128)):\n",
    "        super(VGG16AttentionFeatures, self).__init__()\n",
    "        if not isinstance(img_size, np.ndarray):\n",
    "            img_size = np.array(img_size)\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2, padding=0, dilation=1),\n",
    "            ConvolutionalAttentionBlock(img_size // 2**1, 64, 16),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2, padding=0, dilation=1),\n",
    "            ConvolutionalAttentionBlock(img_size // 2**2, 128, 16),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2, padding=0, dilation=1),\n",
    "            ConvolutionalAttentionBlock(img_size // 2**3, 256, 16),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2, padding=0, dilation=1),\n",
    "            ConvolutionalAttentionBlock(img_size // 2**4, 512, 16),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2, padding=0, dilation=1),\n",
    "            ConvolutionalAttentionBlock(img_size // 2**5, 512, 16),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.features(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "\n",
    "transform = Compose([Resize(128), ToTensor()])\n",
    "train_dataset = CIFAR10('dataset/', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "test_dataset = CIFAR10('dataset/', train=False, transform=transform, download=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "img_size = (128,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, n_epoch, train_loader, test_loader, exp_name):\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "\n",
    "    tb_writer = SummaryWriter('log/{}'.format(exp_name))\n",
    "    running_avg_accuracy = 0\n",
    "    step = 0\n",
    "\n",
    "    epoch_iterator = tqdm(\n",
    "            range(n_epoch),\n",
    "            leave=True,\n",
    "            unit=\"epoch\",\n",
    "            postfix={\"tls\": \"%.4f\" % 1},\n",
    "        )\n",
    "\n",
    "    for epoch in epoch_iterator:\n",
    "        model.eval()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        model.train()\n",
    "        for idx, (inputs, targets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            pred = model(inputs)\n",
    "\n",
    "            loss = criterion(pred, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if idx % 250 == 0:\n",
    "                model.eval()\n",
    "                pred = model(inputs)\n",
    "                predict = torch.argmax(pred, 1)\n",
    "                total = targets.size(0)\n",
    "                correct = torch.eq(predict, targets).sum().double().item()\n",
    "                accuracy = correct / total\n",
    "                running_avg_accuracy = 0.6*running_avg_accuracy + 0.4*accuracy\n",
    "                tb_writer.add_scalar('train/loss', loss.item(), step)\n",
    "                tb_writer.add_scalar('train/accuracy', accuracy, step)\n",
    "                tb_writer.add_scalar('train/running_avg_accuracy', running_avg_accuracy, step)\n",
    "                step += 1\n",
    "\n",
    "                epoch_iterator.set_postfix(tls=\"%.4f\" % loss.item())      \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_loader, 0):\n",
    "                    images_test, labels_test = data\n",
    "                    images_test, labels_test = images_test.to(device), labels_test.to(device)\n",
    "                    pred_test = model(images_test)\n",
    "                    predict = torch.argmax(pred_test, 1)\n",
    "                    total += labels_test.size(0)\n",
    "                    correct += torch.eq(predict, labels_test).sum().double().item()\n",
    "                    \n",
    "            tb_writer.add_scalar('test/accuracy', correct/total, epoch)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_features = VGG16AttentionFeatures((128,128))\n",
    "vgg_16 = VGG(attention_features, num_classes=10, init_weights=False)\n",
    "\n",
    "for m in vgg_16.modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, 0, 0.01)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "train(vgg_16, 50, train_loader, test_loader, 'ConvAttention/CIFAR/cbam_vgg16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = features[:44]\n",
    "\n",
    "a = torch.rand((2, 3, 128, 128))\n",
    "vgg_16(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[31:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.array([128, 128]) // 2**5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('DeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3359c77a731973c13502bd1902dfd07916e9bbb09ad2d14e58e7b1c01c627b1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
