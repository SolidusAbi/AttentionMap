{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Attention Block\n",
    "\n",
    "Propuesta alternativa al CoordinateAttention.\n",
    "\n",
    "Se utiliza kernels de distintos tamaños para obtener descriptores horizontales y verticales, el propósito es obtener descriptores que definan la información espacial de forma complementaria para luego unificar dicha información mediante la adición (o concatenación) de ambos descriptores.\n",
    "\n",
    "De todas formas, no se puede ignorar la información de los canales de cada input. Estos podrían afectar drásticamente a la los descriptores espaciales. Por este motivo, se han aplicado DepthWise Separable convolutions para que cada descriptor no se base toda su información teniendo en cuenta todos los canales, sino un subconjunto de ellos. (Es necesario desarrollar esto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalAttentionBlock(nn.Module):\n",
    "    def __init__(self, img_size: tuple, in_channels: int, reduction_rate: int, groups=True, bias=True) -> None:\n",
    "        super(ConvolutionalAttentionBlock, self).__init__()\n",
    "        out_channels = max(8, in_channels // reduction_rate)\n",
    "        H, W = img_size\n",
    "\n",
    "        self.conv_h = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, (1, W), bias=bias, groups=out_channels if groups else 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        self.conv_w = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, (H, 1), bias=bias, groups=out_channels if groups else 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        self.att = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, in_channels, 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_h = self.conv_h(x) # Height descriptor\n",
    "        x_w = self.conv_w(x) # Width descriptor\n",
    "\n",
    "        # Coordinate attention\n",
    "        coordAtt = self.att(x_h+x_w)\n",
    "        # TODO: Concatenate x_h and x_w\n",
    "        \n",
    "        return coordAtt  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESNet-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Block with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import BasicBlock\n",
    "\n",
    "class ResAttentionBlock(BasicBlock):\n",
    "    def __init__(self, img_size: tuple, inplanes:int, planes:int, stride=1, downsample=None, \n",
    "                att_reduction=8, att_groups=True, att_bias=True, **kargs):\n",
    "                 \n",
    "        super(ResAttentionBlock, self).__init__(inplanes, planes, stride, downsample)\n",
    "\n",
    "        self.attention = ConvolutionalAttentionBlock(img_size, planes, att_reduction, \n",
    "                    groups=att_groups, bias=att_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        att = self.attention(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        res = self.relu((att*out) + identity)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import conv1x1\n",
    "from torchvision.models import ResNet\n",
    "import numpy as np\n",
    "\n",
    "class ResNet_Attention(ResNet):\n",
    "    def __init__(self, img_size:tuple, block:nn.Module, layers:list, num_classes=1000, **kargs):\n",
    "        super(ResNet_Attention, self).__init__(BasicBlock, layers, num_classes, kargs)\n",
    "\n",
    "        if not isinstance(img_size, np.ndarray):\n",
    "            img_size = np.array(img_size)\n",
    "\n",
    "        self.inplanes = 64 # Because in super init it has been set to 512\n",
    "        self.layer1 = self._make_attention_layer(tuple(img_size // (2**2)), block, 64, layers[0], **kargs)\n",
    "        self.layer2 = self._make_attention_layer(tuple(img_size // (2**3)), block, 128, layers[1], stride=2,\n",
    "                                       dilate=False, **kargs)\n",
    "        self.layer3 = self._make_attention_layer(tuple(img_size // (2**4)), block, 256, layers[2], stride=2,\n",
    "                                       dilate=False, **kargs)\n",
    "        self.layer4 = self._make_attention_layer(tuple(img_size // (2**5)), block, 512, layers[3], stride=2,\n",
    "                                       dilate=False, **kargs)\n",
    "        \n",
    "    def _make_attention_layer(self, input_size, block, planes, blocks, stride=1, dilate=False, **kargs):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(input_size, self.inplanes, planes, stride, downsample, \n",
    "                    groups=self.groups, base_width=self.base_width, dilation=previous_dilation,\n",
    "                    norm_layer=norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(input_size, self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import BasicBlock\n",
    "\n",
    "# ResNet-18 config\n",
    "# https://github.com/pytorch/vision/blob/28557e0cfe9113a5285330542264f03e4ba74535/torchvision/models/resnet.py#L649-L670\n",
    "resnet_18 = ResNet_Attention((128,128), ResAttentionBlock, [2,2,2,2], num_classes=1000)\n",
    "result = resnet_18(torch.rand(1,3,128,128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "\n",
    "transform = Compose([Resize(128), ToTensor()])\n",
    "train_dataset = CIFAR100('dataset/', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = CIFAR100('dataset/', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "img_size = (128,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = next(iter(train_loader))\n",
    "resnet_18(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, n_epoch, train_loader, test_loader, exp_name):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "\n",
    "    tb_writer = SummaryWriter('log/{}'.format(exp_name))\n",
    "    running_avg_accuracy = 0\n",
    "    step = 0\n",
    "\n",
    "    epoch_iterator = tqdm(\n",
    "            range(n_epoch),\n",
    "            leave=True,\n",
    "            unit=\"epoch\",\n",
    "            postfix={\"tls\": \"%.4f\" % 1},\n",
    "        )\n",
    "\n",
    "    for epoch in epoch_iterator:\n",
    "        model.eval()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "\n",
    "        model.train()\n",
    "        for idx, (inputs, targets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            pred = model(inputs)\n",
    "\n",
    "            loss = criterion(pred, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if idx % 250 == 0:\n",
    "                model.eval()\n",
    "                pred = model(inputs)\n",
    "                predict = torch.argmax(pred, 1)\n",
    "                total = targets.size(0)\n",
    "                correct = torch.eq(predict, targets).sum().double().item()\n",
    "                accuracy = correct / total\n",
    "                running_avg_accuracy = 0.6*running_avg_accuracy + 0.4*accuracy\n",
    "                tb_writer.add_scalar('train/loss', loss.item(), step)\n",
    "                tb_writer.add_scalar('train/accuracy', accuracy, step)\n",
    "                tb_writer.add_scalar('train/running_avg_accuracy', running_avg_accuracy, step)\n",
    "                step += 1\n",
    "\n",
    "                epoch_iterator.set_postfix(tls=\"%.4f\" % loss.item())      \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_loader, 0):\n",
    "                    images_test, labels_test = data\n",
    "                    images_test, labels_test = images_test.to(device), labels_test.to(device)\n",
    "                    pred_test = model(images_test)\n",
    "                    predict = torch.argmax(pred_test, 1)\n",
    "                    total += labels_test.size(0)\n",
    "                    correct += torch.eq(predict, labels_test).sum().double().item()\n",
    "                    \n",
    "            tb_writer.add_scalar('test/accuracy', correct/total, epoch)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:26<?, ?epoch/s, tls=4.4971]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/abian/Workspace/Thesis/AttentionMap/notebooks/ConvolutionalAttentionBlock.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/abian/Workspace/Thesis/AttentionMap/notebooks/ConvolutionalAttentionBlock.ipynb#ch0000014?line=0'>1</a>\u001b[0m resnet_18 \u001b[39m=\u001b[39m ResNet_Attention((\u001b[39m128\u001b[39m,\u001b[39m128\u001b[39m), ResAttentionBlock, [\u001b[39m2\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m2\u001b[39m], num_classes\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/abian/Workspace/Thesis/AttentionMap/notebooks/ConvolutionalAttentionBlock.ipynb#ch0000014?line=1'>2</a>\u001b[0m train(resnet_18, \u001b[39m50\u001b[39;49m, train_loader, test_loader, \u001b[39m'\u001b[39;49m\u001b[39mConvAttention/CIFAR\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/home/abian/Workspace/Thesis/AttentionMap/notebooks/ConvolutionalAttentionBlock.ipynb Cell 13'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, n_epoch, train_loader, test_loader, exp_name)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abian/Workspace/Thesis/AttentionMap/notebooks/ConvolutionalAttentionBlock.ipynb#ch0000013?line=34'>35</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(pred, targets)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abian/Workspace/Thesis/AttentionMap/notebooks/ConvolutionalAttentionBlock.ipynb#ch0000013?line=35'>36</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/abian/Workspace/Thesis/AttentionMap/notebooks/ConvolutionalAttentionBlock.ipynb#ch0000013?line=36'>37</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abian/Workspace/Thesis/AttentionMap/notebooks/ConvolutionalAttentionBlock.ipynb#ch0000013?line=38'>39</a>\u001b[0m \u001b[39mif\u001b[39;00m idx \u001b[39m%\u001b[39m \u001b[39m250\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abian/Workspace/Thesis/AttentionMap/notebooks/ConvolutionalAttentionBlock.ipynb#ch0000013?line=39'>40</a>\u001b[0m     model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/Workspace/Anaconda/envs/DeepLearning/lib/python3.10/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Workspace/Anaconda/envs/DeepLearning/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Workspace/Anaconda/envs/DeepLearning/lib/python3.10/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[39m# record the step after step update\u001b[39;00m\n\u001b[1;32m    139\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 141\u001b[0m     F\u001b[39m.\u001b[39;49madam(params_with_grad,\n\u001b[1;32m    142\u001b[0m            grads,\n\u001b[1;32m    143\u001b[0m            exp_avgs,\n\u001b[1;32m    144\u001b[0m            exp_avg_sqs,\n\u001b[1;32m    145\u001b[0m            max_exp_avg_sqs,\n\u001b[1;32m    146\u001b[0m            state_steps,\n\u001b[1;32m    147\u001b[0m            amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    148\u001b[0m            beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    149\u001b[0m            beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    150\u001b[0m            lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    151\u001b[0m            weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    152\u001b[0m            eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    153\u001b[0m            maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    154\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Workspace/Anaconda/envs/DeepLearning/lib/python3.10/site-packages/torch/optim/_functional.py:110\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    105\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(bias_correction2))\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    109\u001b[0m step_size \u001b[39m=\u001b[39m lr \u001b[39m/\u001b[39m bias_correction1\n\u001b[0;32m--> 110\u001b[0m param\u001b[39m.\u001b[39;49maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "resnet_18 = ResNet_Attention((128,128), ResAttentionBlock, [2,2,2,2], num_classes=100)\n",
    "train(resnet_18, 50, train_loader, test_loader, 'ConvAttention/CIFAR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('DeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d9b8aa8d774518be7ebcfd06a2463a8035a66798fac49b1a363f570d2d8622e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
